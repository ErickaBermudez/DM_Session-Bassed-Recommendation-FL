{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0ab056c-5493-4326-8753-43e656dde1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import operator\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder\n",
    "from torch.nn import TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec421ed4-7020-4c28-99df-48d08b3138b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial preprocess so it can be used for sr-san\n",
    "# we generate artificial dates based on sequence\n",
    "\n",
    "data = open(\"dataset/computers_sample_train.txt\", \"r\")\n",
    "\n",
    "session_id = 0\n",
    "seq = 1\n",
    "data_array = []\n",
    "\n",
    "def formated_seq(seq): \n",
    "    holder = 1\n",
    "    if seq < 10:\n",
    "        holder = \"0\" + str(seq)\n",
    "    else:\n",
    "        holder = str(seq)\n",
    "    return holder\n",
    "\n",
    "for line in data:\n",
    "    for micro in line.split(\" \"): \n",
    "        if seq < 30:\n",
    "            \n",
    "            day = \"+2001-01-\" + formated_seq(seq)\n",
    "        elif seq < 60:\n",
    "             day = \"+2001-02-0\" + formated_seq(seq-29)\n",
    "        else:\n",
    "            day = \"+2001-03-0\" + str(np.random.randint(1,30))\n",
    "        line = str(session_id) + \"+NA+\" + micro + day;\n",
    "        line = line.strip(\"\\n \")\n",
    "        data_array.append(line)\n",
    "        seq += 1\n",
    "    seq = 1\n",
    "    session_id += 1\n",
    "    if session_id > 100: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d800b48d-8c29-49dc-b8a1-f24b1afe9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we produce a data frame to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0ce9d0-31da-4b1a-aadf-cee527516364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_item</th>\n",
       "      <th>behavior_type</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>dwell_time</th>\n",
       "      <th>eventdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>2885143</td>\n",
       "      <td>5</td>\n",
       "      <td>683</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2001-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>3282626</td>\n",
       "      <td>5</td>\n",
       "      <td>679</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>2001-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>3282626</td>\n",
       "      <td>7</td>\n",
       "      <td>679</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>2001-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>3203662</td>\n",
       "      <td>5</td>\n",
       "      <td>679</td>\n",
       "      <td>191</td>\n",
       "      <td>10</td>\n",
       "      <td>2001-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>1836048</td>\n",
       "      <td>5</td>\n",
       "      <td>681</td>\n",
       "      <td>122</td>\n",
       "      <td>9</td>\n",
       "      <td>2001-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>100</td>\n",
       "      <td>NA</td>\n",
       "      <td>2948633</td>\n",
       "      <td>6</td>\n",
       "      <td>688</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>2001-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>100</td>\n",
       "      <td>NA</td>\n",
       "      <td>1250967</td>\n",
       "      <td>5</td>\n",
       "      <td>2694</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>2001-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2859</th>\n",
       "      <td>100</td>\n",
       "      <td>NA</td>\n",
       "      <td>2131674</td>\n",
       "      <td>5</td>\n",
       "      <td>2694</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>2001-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2860</th>\n",
       "      <td>100</td>\n",
       "      <td>NA</td>\n",
       "      <td>1892003</td>\n",
       "      <td>5</td>\n",
       "      <td>2694</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>2001-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>100</td>\n",
       "      <td>NA</td>\n",
       "      <td>1250967</td>\n",
       "      <td>5</td>\n",
       "      <td>2694</td>\n",
       "      <td>13</td>\n",
       "      <td>13\\n</td>\n",
       "      <td>2001-01-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2862 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     session_id user_id item_item behavior_type item_id timeframe dwell_time  \\\n",
       "0             0      NA   2885143             5     683         9          4   \n",
       "1             0      NA   3282626             5     679         3         52   \n",
       "2             0      NA   3282626             7     679        49         52   \n",
       "3             0      NA   3203662             5     679       191         10   \n",
       "4             0      NA   1836048             5     681       122          9   \n",
       "...         ...     ...       ...           ...     ...       ...        ...   \n",
       "2857        100      NA   2948633             6     688       139        137   \n",
       "2858        100      NA   1250967             5    2694        21         21   \n",
       "2859        100      NA   2131674             5    2694        51         51   \n",
       "2860        100      NA   1892003             5    2694        20         20   \n",
       "2861        100      NA   1250967             5    2694        13       13\\n   \n",
       "\n",
       "       eventdate  \n",
       "0     2001-01-01  \n",
       "1     2001-01-02  \n",
       "2     2001-01-03  \n",
       "3     2001-01-04  \n",
       "4     2001-01-05  \n",
       "...          ...  \n",
       "2857  2001-01-18  \n",
       "2858  2001-01-19  \n",
       "2859  2001-01-20  \n",
       "2860  2001-01-21  \n",
       "2861  2001-01-22  \n",
       "\n",
       "[2862 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pf = pd.DataFrame(data_array)\n",
    "data_pf.columns = [\"data\"]\n",
    "data_pf = data_pf.data.str.split(pat='+',expand=True)\n",
    "data_pf.columns = [\"session_id\",  \"user_id\", \"item_item\", \"behavior_type\", \"item_id\", \"timeframe\", \"dwell_time\", \"eventdate\"]\n",
    "data_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b2fcde8-98e3-46e2-9081-190c35dd455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unused columns\n",
    "del(data_pf['behavior_type'])\n",
    "del(data_pf['item_item'])\n",
    "del(data_pf['dwell_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35a68cf6-b38e-429f-8115-06bedcf1ff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Saved data\n"
     ]
    }
   ],
   "source": [
    "# save it\n",
    "from pathlib import Path\n",
    "filepath = Path('jd_computers.csv', index=False)  \n",
    "data_pf.to_csv(filepath) \n",
    "print(\"--> Saved data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3d9b3e-dc59-4ed3-a2f2-dcb988c24ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "####################################################################################\n",
    "####### SR-SAN CODE OBTAINED FROM https://github.com/GalaxyCruiser/SR-SAN ##########\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "# PREPROCESS SR-SAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceab3d3f-deb5-40cb-b7d3-fc1feafc2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Starting @ 2022-06-17 19:45:36.259948s\n",
      "-- Reading data @ 2022-06-17 19:45:36.275980s\n",
      "Splitting date 1608822000.0\n",
      "39\n",
      "37\n",
      "[('7', 979657200.0), ('68', 979657200.0), ('55', 979743600.0)]\n",
      "[('0', 1609426800.0), ('1', 1609426800.0), ('3', 1609426800.0)]\n",
      "-- Splitting train set and test set @ 2022-06-17 19:45:36.277978s\n",
      "84\n",
      "299\n",
      "225\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 8, 3], [1, 2, 3, 4, 5, 6, 7, 8, 8], [1, 2, 3, 4, 5, 6, 7, 8]] [979657200.0, 979657200.0, 979657200.0] [9, 3, 8]\n",
      "[[57, 77, 77, 77], [57, 77, 77], [57, 77]] [1609426800.0, 1609426800.0, 1609426800.0] [77, 77, 77]\n",
      "avg length:  8.594202898550725\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import operator\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "dataset = 'jd_computers.csv'\n",
    "\n",
    "print(\"-- Starting @ %ss\" % datetime.datetime.now())\n",
    "with open(dataset, \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    sess_clicks = {}\n",
    "    sess_date = {}\n",
    "    ctr = 0\n",
    "    curid = -1\n",
    "    curdate = None\n",
    "    for data in reader:\n",
    "        sessid = data['session_id']\n",
    "        if curdate and not curid == sessid:\n",
    "            date = ''\n",
    "            try:\n",
    "                date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
    "            except: \n",
    "                date = time.mktime(time.strptime(\"2021-01-01\", '%Y-%m-%d'))\n",
    "                \n",
    "            sess_date[curid] = date\n",
    "        curid = sessid\n",
    "        try:\n",
    "            item = data['item_id'], int(data['timeframe'])\n",
    "        except: \n",
    "            item = data['item_id'], int(1)\n",
    "        curdate = ''\n",
    "        curdate = data['eventdate']\n",
    "\n",
    "        if sessid in sess_clicks:\n",
    "            sess_clicks[sessid] += [item]\n",
    "        else:\n",
    "            sess_clicks[sessid] = [item]\n",
    "        ctr += 1\n",
    "    date = ''\n",
    "    try:\n",
    "        date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
    "    except: \n",
    "        date = time.mktime(time.strptime(\"2021-01-01\", '%Y-%m-%d'))\n",
    "        for i in list(sess_clicks):\n",
    "            sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n",
    "            sess_clicks[i] = [c[0] for c in sorted_clicks]\n",
    "    sess_date[curid] = date\n",
    "print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n",
    "\n",
    "# Filter out length 1 sessions\n",
    "for s in list(sess_clicks):\n",
    "    if len(sess_clicks[s]) == 1:\n",
    "        del sess_clicks[s]\n",
    "        del sess_date[s]\n",
    "\n",
    "# Count number of times each item appears\n",
    "iid_counts = {}\n",
    "for s in sess_clicks:\n",
    "    seq = sess_clicks[s]\n",
    "    for iid in seq:\n",
    "        if iid in iid_counts:\n",
    "            iid_counts[iid] += 1\n",
    "        else:\n",
    "            iid_counts[iid] = 1\n",
    "\n",
    "sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n",
    "\n",
    "length = len(sess_clicks)\n",
    "for s in list(sess_clicks):\n",
    "    curseq = sess_clicks[s]\n",
    "    filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n",
    "    if len(filseq) < 2:\n",
    "        del sess_clicks[s]\n",
    "        del sess_date[s]\n",
    "    else:\n",
    "        sess_clicks[s] = filseq\n",
    "\n",
    "# Split out test set based on dates\n",
    "dates = list(sess_date.items())\n",
    "maxdate = dates[0][1]\n",
    "\n",
    "for _, date in dates:\n",
    "    if maxdate < date:\n",
    "        maxdate = date\n",
    "\n",
    "# 7 days for test\n",
    "splitdate = 0\n",
    "splitdate = maxdate - 86400 * 7\n",
    "\n",
    "print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n",
    "tra_sess = filter(lambda x: x[1] < splitdate, dates)\n",
    "tes_sess = filter(lambda x: x[1] > splitdate, dates)\n",
    "\n",
    "# Sort sessions by date\n",
    "tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
    "tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
    "print(len(tra_sess))    # 186670    # 7966257\n",
    "print(len(tes_sess))    # 15979     # 15324\n",
    "print(tra_sess[:3])\n",
    "print(tes_sess[:3])\n",
    "print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n",
    "\n",
    "# Choosing item count >=5 gives approximately the same number of items as reported in paper\n",
    "item_dict = {}\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_ids = []\n",
    "    train_seqs = []\n",
    "    train_dates = []\n",
    "    item_ctr = 1\n",
    "    for s, date in tra_sess:\n",
    "        seq = sess_clicks[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_ids += [s]\n",
    "        train_dates += [date]\n",
    "        train_seqs += [outseq]\n",
    "    print(item_ctr)     # 43098, 37484\n",
    "    return train_ids, train_dates, train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_ids = []\n",
    "    test_seqs = []\n",
    "    test_dates = []\n",
    "    for s, date in tes_sess:\n",
    "        seq = sess_clicks[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_ids += [s]\n",
    "        test_dates += [date]\n",
    "        test_seqs += [outseq]\n",
    "    return test_ids, test_dates, test_seqs\n",
    "\n",
    "\n",
    "tra_ids, tra_dates, tra_seqs = obtian_tra()\n",
    "tes_ids, tes_dates, tes_seqs = obtian_tes()\n",
    "\n",
    "\n",
    "def process_seqs(iseqs, idates):\n",
    "    out_seqs = []\n",
    "    out_dates = []\n",
    "    labs = []\n",
    "    ids = []\n",
    "    for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_dates += [date]\n",
    "            ids += [id]\n",
    "    return out_seqs, out_dates, labs, ids\n",
    "\n",
    "\n",
    "tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n",
    "te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(len(tr_seqs))\n",
    "print(len(te_seqs))\n",
    "print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n",
    "print(te_seqs[:3], te_dates[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "if not os.path.exists('jd'):\n",
    "    os.makedirs('jd')\n",
    "pickle.dump(tra, open('jd/train.txt', 'wb'))\n",
    "pickle.dump(tes, open('jd/test.txt', 'wb'))\n",
    "pickle.dump(tra_seqs, open('jd/all_train_seq.txt', 'wb'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9b38a4-7d80-40dd-b028-29c63d82d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes to use = 84\n"
     ]
    }
   ],
   "source": [
    "# SR-SAN MAIN CODE\n",
    "# above we can see the amount of nodes already, but we can make sure\n",
    "data = open(\"jd/train.txt\", \"r\")\n",
    "obj = pd.read_pickle('jd/train.txt')\n",
    "df = pd.DataFrame(obj)\n",
    "nodes = df.max(axis=1)[1] + 1 \n",
    "print(\"Nodes to use = \" + str(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a98618d7-9c85-4805-9bba-e6cd14ddb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "\n",
    "def data_masks(all_usr_pois, item_tail):\n",
    "    us_lens = [len(upois) for upois in all_usr_pois]\n",
    "    len_max = max(us_lens)\n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "    return us_pois, us_msks, len_max\n",
    "\n",
    "\n",
    "def split_validation(train_set, valid_portion):\n",
    "    train_set_x, train_set_y = train_set\n",
    "    n_samples = len(train_set_x)\n",
    "    sidx = np.arange(n_samples, dtype='int32')\n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
    "\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, data, shuffle=False, graph=None):\n",
    "        inputs = data[0]\n",
    "        inputs, mask, len_max = data_masks(inputs, [0])\n",
    "        self.inputs = np.asarray(inputs)\n",
    "        self.mask = np.asarray(mask)\n",
    "        self.len_max = len_max\n",
    "        self.targets = np.asarray(data[1])\n",
    "        self.length = len(inputs)\n",
    "        self.shuffle = shuffle\n",
    "        self.graph = graph\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            shuffled_arg = np.arange(self.length)\n",
    "            np.random.shuffle(shuffled_arg)\n",
    "            self.inputs = self.inputs[shuffled_arg]\n",
    "            self.mask = self.mask[shuffled_arg]\n",
    "            self.targets = self.targets[shuffled_arg]\n",
    "        n_batch = int(self.length / batch_size)\n",
    "        if self.length % batch_size != 0:\n",
    "            n_batch += 1\n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
    "        return slices\n",
    "\n",
    "    def get_slice(self, i):\n",
    "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
    "        items, n_node, A, alias_inputs = [], [], [], []\n",
    "        for u_input in inputs:\n",
    "            n_node.append(len(np.unique(u_input)))\n",
    "        max_n_node = np.max(n_node)\n",
    "        for u_input in inputs:\n",
    "            node = np.unique(u_input)\n",
    "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "            u_A = np.zeros((max_n_node, max_n_node))\n",
    "            for i in np.arange(len(u_input) - 1):\n",
    "                if u_input[i + 1] == 0:\n",
    "                    break\n",
    "                u = np.where(node == u_input[i])[0][0]\n",
    "                v = np.where(node == u_input[i + 1])[0][0]\n",
    "                u_A[u][v] = 1\n",
    "            u_sum_in = np.sum(u_A, 0)\n",
    "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "            u_A_in = np.divide(u_A, u_sum_in)\n",
    "            u_sum_out = np.sum(u_A, 1)\n",
    "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "            A.append(u_A)\n",
    "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "        return alias_inputs, A, items, mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9015c4eb-3477-43aa-a8ef-e4e1b438cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model\n",
    "class SelfAttentionNetwork(Module):\n",
    "    def __init__(self, opt, n_node):\n",
    "        super(SelfAttentionNetwork, self).__init__()\n",
    "        self.hidden_size = opt.hiddenSize\n",
    "        self.n_node = n_node\n",
    "        self.batch_size = opt.batchSize\n",
    "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
    "        self.transformerEncoderLayer = TransformerEncoderLayer(d_model=self.hidden_size, nhead=opt.nhead,dim_feedforward=self.hidden_size * opt.feedforward)\n",
    "        self.transformerEncoder = TransformerEncoder(self.transformerEncoderLayer, opt.layer)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def compute_scores(self, hidden, mask):\n",
    "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
    "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
    "        scores = torch.matmul(ht, b.transpose(1, 0))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, inputs, A):\n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = hidden.transpose(0,1).contiguous()\n",
    "        hidden = self.transformerEncoder(hidden)\n",
    "        hidden = hidden.transpose(0,1).contiguous()\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "def forward(model, i, data):\n",
    "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    A = trans_to_cuda(torch.Tensor(A).float())\n",
    "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "    hidden = model(items, A)\n",
    "    get = lambda i: hidden[i][alias_inputs[i]]\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "    return targets, model.compute_scores(seq_hidden, mask)\n",
    "\n",
    "\n",
    "def train_test(model, train_data, test_data):    \n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        model.optimizer.zero_grad()\n",
    "        targets, scores = forward(model, i, train_data)\n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets - 1)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        total_loss += loss\n",
    "        if j % int(len(slices) / 5 + 1) == 0:\n",
    "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
    "    print('\\tLoss:\\t%.3f' % total_loss)\n",
    "\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        sub_scores = scores.topk(5)[1]\n",
    "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    model.scheduler.step()\n",
    "    return hit, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e287cf-069e-4ba1-bef2-c328756a62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt[\"batchSize\"] = 100 # input batch size\n",
    "#opt[\"hiddenSize\"] = 96 # hidden state size   \n",
    "#opt[\"nhead\"] = 2 # the number of heads of multi-head attention    \n",
    "#opt[\"layer\"] = 1 # number of SAN layers   \n",
    "#opt[\"feedforward\"] = 4 # the multiplier of hidden state size   \n",
    "#opt[\"epoch\"] = 12 # the number of epochs to train for\n",
    "#opt[\"lr\"] = 0.001 # learning rate\n",
    "#opt[\"lr_dc\"] = 0.1 # learning rate decay rate\n",
    "#opt[\"lr_dc_step\"] = 3 # the number of steps after which the learning rate decay\n",
    "#opt[\"l2\"] = 1e-5 # l2 penalty\n",
    "#opt[\"patience\"] = 10 # the number of epoch to wait before early stop\n",
    "#opt[\"validation\"] = \"store_true\" # validatiob\n",
    "#opt[\"valid_portion\"] = 0.1 #split the portion of training set as validation set\n",
    "\n",
    "class Opt:\n",
    "    dataset = 'jd'\n",
    "    batchSize = 100\n",
    "    hiddenSize = 96\n",
    "    nhead = 2\n",
    "    layer = 1\n",
    "    feedforward = 4\n",
    "    epoch = 12\n",
    "    lr = 0.001\n",
    "    lr_dc = 0.1\n",
    "    lr_dc_step = 3\n",
    "    l2 = 1e-5\n",
    "    patience = 10\n",
    "    validation = 'store_true'\n",
    "    valid_portion = 0.1\n",
    "    \n",
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92665021-b8e0-4c98-b3ec-42260cc35d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_data = pickle.load(open(opt.dataset + '/train.txt', 'rb'))\n",
    "    if opt.validation:\n",
    "        train_data, valid_data = split_validation(train_data, opt.valid_portion)\n",
    "        test_data = valid_data\n",
    "    else:\n",
    "        test_data = pickle.load(open(opt.dataset + '/test.txt', 'rb'))\n",
    "\n",
    "    train_data = Data(train_data, shuffle=True)\n",
    "    test_data = Data(test_data, shuffle=False)\n",
    "    n_node = 85\n",
    "\n",
    "\n",
    "    model = trans_to_cuda(SelfAttentionNetwork(opt, n_node))\n",
    "\n",
    "    start = time.time()\n",
    "    best_result = [0, 0]\n",
    "    best_epoch = [0, 0]\n",
    "    bad_counter = 0\n",
    "    for epoch in range(opt.epoch):\n",
    "        print('-------------------------------------------------------')\n",
    "        print('epoch: ', epoch)\n",
    "        hit, mrr = train_test(model, train_data, test_data)\n",
    "        flag = 0\n",
    "        if hit >= best_result[0]:\n",
    "            best_result[0] = hit\n",
    "            best_epoch[0] = epoch\n",
    "            flag = 1\n",
    "        if mrr >= best_result[1]:\n",
    "            best_result[1] = mrr\n",
    "            best_epoch[1] = epoch\n",
    "            flag = 1\n",
    "        print('Best Result:')\n",
    "        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
    "        bad_counter += 1 - flag\n",
    "        if bad_counter >= opt.patience:\n",
    "            break\n",
    "    print('-------------------------------------------------------')\n",
    "    end = time.time()\n",
    "    print(\"Run time: %f s\" % (end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e813feeb-4b77-442c-b528-e6da216bd09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "epoch:  0\n",
      "start training:  2022-06-17 19:46:00.091034\n",
      "[0/3] Loss: 4.4471\n",
      "[1/3] Loss: 4.4324\n",
      "[2/3] Loss: 4.4272\n",
      "\tLoss:\t13.307\n",
      "start predicting:  2022-06-17 19:46:00.226034\n",
      "Best Result:\n",
      "\tRecall@20:\t6.6667\tMMR@20:\t1.7778\tEpoch:\t0,\t0\n",
      "-------------------------------------------------------\n",
      "epoch:  1\n",
      "start training:  2022-06-17 19:46:00.236033\n",
      "[0/3] Loss: 4.4101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferimaroo\\AppData\\Local\\Temp\\ipykernel_17576\\834171849.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  A = trans_to_cuda(torch.Tensor(A).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] Loss: 4.4237\n",
      "[2/3] Loss: 4.4135\n",
      "\tLoss:\t13.247\n",
      "start predicting:  2022-06-17 19:46:00.346035\n",
      "Best Result:\n",
      "\tRecall@20:\t10.0000\tMMR@20:\t2.7778\tEpoch:\t1,\t1\n",
      "-------------------------------------------------------\n",
      "epoch:  2\n",
      "start training:  2022-06-17 19:46:00.356035\n",
      "[0/3] Loss: 4.3979\n",
      "[1/3] Loss: 4.4007\n",
      "[2/3] Loss: 4.3966\n",
      "\tLoss:\t13.195\n",
      "start predicting:  2022-06-17 19:46:00.470033\n",
      "Best Result:\n",
      "\tRecall@20:\t10.0000\tMMR@20:\t3.6111\tEpoch:\t2,\t2\n",
      "-------------------------------------------------------\n",
      "epoch:  3\n",
      "start training:  2022-06-17 19:46:00.480064\n",
      "[0/3] Loss: 4.3956\n",
      "[1/3] Loss: 4.3826\n",
      "[2/3] Loss: 4.3812\n",
      "\tLoss:\t13.159\n",
      "start predicting:  2022-06-17 19:46:00.598033\n",
      "Best Result:\n",
      "\tRecall@20:\t10.0000\tMMR@20:\t3.6111\tEpoch:\t3,\t3\n",
      "-------------------------------------------------------\n",
      "epoch:  4\n",
      "start training:  2022-06-17 19:46:00.608035\n",
      "[0/3] Loss: 4.3852\n",
      "[1/3] Loss: 4.3824\n",
      "[2/3] Loss: 4.3904\n",
      "\tLoss:\t13.158\n",
      "start predicting:  2022-06-17 19:46:00.719033\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  5\n",
      "start training:  2022-06-17 19:46:00.729036\n",
      "[0/3] Loss: 4.3854\n",
      "[1/3] Loss: 4.3905\n",
      "[2/3] Loss: 4.3767\n",
      "\tLoss:\t13.153\n",
      "start predicting:  2022-06-17 19:46:00.847032\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  6\n",
      "start training:  2022-06-17 19:46:00.857033\n",
      "[0/3] Loss: 4.3969\n",
      "[1/3] Loss: 4.3699\n",
      "[2/3] Loss: 4.3841\n",
      "\tLoss:\t13.151\n",
      "start predicting:  2022-06-17 19:46:00.973034\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  7\n",
      "start training:  2022-06-17 19:46:00.983033\n",
      "[0/3] Loss: 4.3820\n",
      "[1/3] Loss: 4.3801\n",
      "[2/3] Loss: 4.3924\n",
      "\tLoss:\t13.155\n",
      "start predicting:  2022-06-17 19:46:01.094035\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  8\n",
      "start training:  2022-06-17 19:46:01.104035\n",
      "[0/3] Loss: 4.3718\n",
      "[1/3] Loss: 4.3897\n",
      "[2/3] Loss: 4.3893\n",
      "\tLoss:\t13.151\n",
      "start predicting:  2022-06-17 19:46:01.230033\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  9\n",
      "start training:  2022-06-17 19:46:01.240036\n",
      "[0/3] Loss: 4.3842\n",
      "[1/3] Loss: 4.3823\n",
      "[2/3] Loss: 4.3792\n",
      "\tLoss:\t13.146\n",
      "start predicting:  2022-06-17 19:46:01.354035\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  10\n",
      "start training:  2022-06-17 19:46:01.364034\n",
      "[0/3] Loss: 4.3890\n",
      "[1/3] Loss: 4.3799\n",
      "[2/3] Loss: 4.3826\n",
      "\tLoss:\t13.152\n",
      "start predicting:  2022-06-17 19:46:01.478034\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "epoch:  11\n",
      "start training:  2022-06-17 19:46:01.490034\n",
      "[0/3] Loss: 4.3774\n",
      "[1/3] Loss: 4.3802\n",
      "[2/3] Loss: 4.3965\n",
      "\tLoss:\t13.154\n",
      "start predicting:  2022-06-17 19:46:01.604035\n",
      "Best Result:\n",
      "\tRecall@20:\t13.3333\tMMR@20:\t4.5556\tEpoch:\t4,\t4\n",
      "-------------------------------------------------------\n",
      "Run time: 1.524002 s\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a7cda-54d8-439e-a5d1-ba248670e700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e4e3a-0f30-40c2-8740-52a51aba7d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
